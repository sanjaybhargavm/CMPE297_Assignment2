{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotharisanjana/CMPE297_SpecialTopics_Fall2023/blob/main/Assignment_2/NanoGPT_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "x9t529IT580a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1p4m55mtQca",
        "outputId": "3f2c223e-d20a-4de3-8f74-5200a28960dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(101)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WszbuVk6Hg-",
        "outputId": "37e6b001-2f1e-4a9c-ef44-feaacba955fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7db15d9cfb70>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 500\n",
        "n_embd = 64\n",
        "n_head = 8\n",
        "n_layer = 8"
      ],
      "metadata": {
        "id": "DeADGpnw6C9j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"/content/drive/MyDrive/CMPE297/burining_whales.txt\""
      ],
      "metadata": {
        "id": "k5imSmEatdK1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 0\n",
        "    self.train_data = torch.tensor([])\n",
        "    self.val_data = torch.tensor([])\n",
        "\n",
        "  def read_dataset(self):\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        self.data = f.read()\n",
        "\n",
        "  def prepare_dataset(self):\n",
        "    chars = sorted(list(set(self.data)))\n",
        "    self.vocab_size = len(chars)\n",
        "    char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "    int_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "    self.encode = lambda s: [char_to_int[c] for c in s]\n",
        "    self.decode = lambda l: ''.join([int_to_char[i] for i in l])\n",
        "\n",
        "  def data_split(self):\n",
        "    data_tensor = torch.tensor(self.encode(self.data), dtype=torch.long)\n",
        "    n = int(0.8*len(data_tensor))\n",
        "    self.train_data = data_tensor[:n]\n",
        "    self.val_data = data_tensor[n:]\n",
        "\n",
        "  def get_batch(self, split):\n",
        "    data = self.train_data if split == 'train' else self.val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rrd7yijpOcIZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetObj = Dataset()\n",
        "datasetObj.read_dataset()\n",
        "datasetObj.prepare_dataset()\n",
        "datasetObj.data_split()"
      ],
      "metadata": {
        "id": "oSlIFHNmuGDV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  @torch.no_grad()\n",
        "  def estimate_loss(self):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = datasetObj.get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "kYGFSWWJRqfb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossObj = Loss()"
      ],
      "metadata": {
        "id": "vuFmZim7um45"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    w = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    w = F.softmax(w, dim=-1)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = w @ v\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "bTVi1D-uSKQT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "IPrnCUFiUCB5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "PMABKHfSUJX6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedFoward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "aV8Trt7HUVkE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoGPT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(datasetObj.vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, datasetObj.vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "6wlMV8JYUjiL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateNext():\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  print(datasetObj.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "dyCbV9XLWH5q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TozLG9XW53XR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe90c1b-65f3-4373-dba9-400b8dcefaa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.405027 M parameters\n",
            "step 0: train loss 3.7954, val loss 3.7279\n",
            "step 1000: train loss 0.1124, val loss 5.6521\n",
            "step 2000: train loss 0.1071, val loss 5.9051\n",
            "step 3000: train loss 0.0980, val loss 6.1296\n",
            "step 4000: train loss 0.0999, val loss 6.3486\n",
            "step 4999: train loss 0.0969, val loss 6.4930\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = NanoGPT()\n",
        "  m = model.to(device)\n",
        "  print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = lossObj.estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = datasetObj.get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generateNext()"
      ],
      "metadata": {
        "id": "6oEiOyZyWNGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d57a86-5426-403e-ac40-9db817172bd0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "If we were but fire we would not mind mither.\n",
            "\n",
            "But we are also water, even one drop of which forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the forld ming wh the ithe the corpses.\n",
            "\n",
            "If we were but fire we would not mind either.\n",
            "\n",
            "But we are also water, even one drop of which forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the f ich ared bed whith baby sleeping.\n",
            "\n",
            "So it was with the whales beached in Oregon and the not knowing what else to do with the corpses.\n",
            "\n",
            "If we were but fire we would not mind either.\n",
            "\n",
            "But we are also water, even one drop of which forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the forlich f a's tes.\n",
            "\n",
            "If we were but fire we would not mind either.\n",
            "\n",
            "But we are also water, even one drop of which forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the f ich ared sor.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the fores mich g.\n",
            "\n",
            "Th we whale there in Ore on and the not knowing what else to do with the corpses.\n",
            "\n",
            "If we were but fire we would not mind either.\n",
            "\n",
            "But we are also water, even one drop of which forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the forlit s the mach and beheld the f ich forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the fnore mich and beheld the foregops.\n",
            "\n",
            "If we were but fire we would not mind either.\n",
            "\n",
            "But we are also water, even one drop of which forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the forlich and t behe wich forms and thear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the forld mich g forms a tear.\n",
            "\n",
            "Those who were there that day, who lit the match and beheld the f ich ared s.\n",
            "\n",
            "So withe whale where that day, who lit the match and beheld the forlit se thes whe corlich and behed wherego t day, with the ch formses.\n",
            "\n",
            "If we were but fire we would not mind either.\n",
            "\n",
            "But we are also water, even one drop of whi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7vqctsBGZ_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}